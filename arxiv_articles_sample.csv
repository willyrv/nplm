id|title|authors|arxiv_primary_category|summary|published|updated|general_category
http://arxiv.org/abs/1502.02721v1|In-situ measurements of the radiation stability of amino acids at 15-140   K|P. A. Gerakines;R. L. Hudson;M. H. Moore;J. -L. Bell|astro-ph.IM|We present new kinetics data on the radiolytic destruction of amino acids measured in situ with infrared spectroscopy. Samples were irradiated at 15, 100, and 140 K with 0.8-MeV protons, and amino-acid decay was followed at each temperature with and without H$_2$O present. Observed radiation products included CO$_2$ and amines, consistent with amino-acid decarboxylation. The half-lives of glycine, alanine, and phenylalanine were estimated for various extraterrestrial environments. Infrared spectral changes demonstrated the conversion from the non-zwitterion structure NH$_2$-CH$_2$(R)-COOH at 15 K to the zwitterion structure $^+$NH$_3$-CH$_2$(R)-COO$^-$ at 140 K for each amino acid studied.|2015-02-09T23:03:10Z|2015-02-09T23:03:10Z|astro-ph
http://arxiv.org/abs/1503.01540v1|Finding meteorite impacts in Aboriginal oral tradition|Duane W. Hamacher|physics.hist-ph|Aboriginal stories dating back many thousands of years talk of a fire from the sky in an area now home to the Henbury meteorite craters, in the Northern Territory|2015-03-05T05:21:09Z|2015-03-05T05:21:09Z|physics
http://arxiv.org/abs/1909.09824v1|Desperate times call for desperate measures: government spending   multipliers in hard times|Sokbae Lee;Yuan Liao;Myung Hwan Seo;Youngki Shin|econ.GN|We investigate state-dependent effects of fiscal multipliers and allow for endogenous sample splitting to determine whether the US economy is in a slack state. When the endogenized slack state is estimated as the period of the unemployment rate higher than about 12 percent, the estimated cumulative multipliers are significantly larger during slack periods than non-slack periods and are above unity. We also examine the possibility of time-varying regimes of slackness and find that our empirical results are robust under a more flexible framework. Our estimation results points out the importance of the heterogenous effects of fiscal policy.|2019-09-21T13:42:17Z|2019-09-21T13:42:17Z|econ
http://arxiv.org/abs/1809.07292v2|Online control of the false discovery rate in biomedical research|David S. Robertson;James M. S. Wason|stat.ME|Modern biomedical research frequently involves testing multiple related hypotheses, while maintaining control over a suitable error rate. In many applications the false discovery rate (FDR), which is the expected proportion of false positives among the rejected hypotheses, has become the standard error criterion. Procedures that control the FDR, such as the well-known Benjamini-Hochberg procedure, assume that all p-values are available to be tested at a single time point. However, this ignores the sequential nature of many biomedical experiments, where a sequence of hypotheses is tested without having access to future p-values or even the number of hypotheses. Recently, the first procedures that control the FDR in this online manner have been proposed by Javanmard and Montanari (Ann. Stat. 2018), and built upon by Ramdas et al. (NIPS 2017, ICML 2018). In this paper, we compare and contrast these proposed procedures, with a particular focus on the setting where the p-values are dependent. We also propose a simple modification of the procedures for when there is an upper bound on the number of hypotheses to be tested. Using comprehensive simulation scenarios and case studies, we provide recommendations for which procedures to use in practice for online FDR control.|2018-09-19T16:37:46Z|2018-09-26T17:08:01Z|stat
http://arxiv.org/abs/1902.10021v1|Gig Economy: A Dynamic Principal-Agent Model|Zsolt Bihary;Péter Kerényi|econ.GN|The gig economy, where employees take short-term, project-based jobs, is increasingly spreading all over the world. In this paper, we investigate the employer's and the worker's behavior in the gig economy with a dynamic principal-agent model. In our proposed model the worker's previous decisions influence his later decisions through his dynamically changing participation constraint. He accepts the contract offered by the employer when his expected utility is higher than the irrational valuation of his effort's worth. This reference point is based on wages he achieved in previous rounds. We formulate the employer's stochastic control problem and derive the solution in the deterministic limit. We obtain the feasible net wage of the worker, and the profit of the employer. Workers who can afford to go unemployed and need not take a gig at all costs will realize high net wages. Conversely, far-sighted employers who can afford to stall production will obtain high profits.|2019-02-26T16:05:06Z|2019-02-26T16:05:06Z|econ
http://arxiv.org/abs/1511.03159v3|On the C-property and $w^*$-representations of risk measures|Niushan Gao;Foivos Xanthos|q-fin.MF|We identify a large class of Orlicz spaces $X$ for which the topology $\sigma(X,X_n^\sim)$ fails the C-property introduced in [7]. We also establish a variant of the C-property and use it to prove a $w^*$-representation theorem for proper convex increasing functionals on dual Banach lattices that satisfy a suitable version of Delbaen's Fatou property. Our results apply, in particular, to risk measures on all Orlicz spaces over $[0,1]$ which is not $L_1[0,1]$.|2015-11-10T16:01:28Z|2016-09-21T03:39:00Z|q-fin
http://arxiv.org/abs/1801.04080v1|Optimal contracts under competition when uncertainty from adverse   selection and moral hazard are present|N. Packham|q-fin.PM|In a continuous-time setting where a risk-averse agent controls the drift of an output process driven by a Brownian motion, optimal contracts are linear in the terminal output; this result is well-known in a setting with moral hazard and -under stronger assumptions - adverse selection. We show that this result continues to hold when in addition reservation utilities are type-dependent. This type of problem occurs in the study of optimal compensation problems involving competing principals.|2018-01-12T08:10:20Z|2018-01-12T08:10:20Z|q-fin
http://arxiv.org/abs/1705.01204v1|Spectral clustering in the dynamic stochastic block model|Marianna Pensky;Teng Zhang|stat.ME|In the present paper, we studied a Dynamic Stochastic Block Model (DSBM) under the assumptions that the connection probabilities, as functions of time, are smooth and that at most $s$ nodes can switch their class memberships between two consecutive time points. We estimate the edge probability tensor by a kernel-type procedure and extract the group memberships of the nodes by spectral clustering. The procedure is computationally viable, adaptive to the unknown smoothness of the functional connection probabilities, to the rate $s$ of membership switching and to the unknown number of clusters. In addition, it is accompanied by non-asymptotic guarantees for the precision of estimation and clustering.|2017-05-02T23:55:26Z|2017-05-02T23:55:26Z|stat
http://arxiv.org/abs/1710.10967v3|Artificial Intelligence as Structural Estimation: Economic   Interpretations of Deep Blue, Bonanza, and AlphaGo|Mitsuru Igami|econ.EM|"Artificial intelligence (AI) has achieved superhuman performance in a growing number of tasks, but understanding and explaining AI remain challenging. This paper clarifies the connections between machine-learning algorithms to develop AIs and the econometrics of dynamic structural models through the case studies of three famous game AIs. Chess-playing Deep Blue is a calibrated value function, whereas shogi-playing Bonanza is an estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's ""supervised-learning policy network"" is a deep neural network implementation of Hotz and Miller's (1993) conditional choice probability estimation; its ""reinforcement-learning value network"" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional choice simulation method. Relaxing these AIs' implicit econometric assumptions would improve their structural interpretability."|2017-10-30T14:25:39Z|2018-03-01T20:52:33Z|econ
http://arxiv.org/abs/0902.1323v1|Sparse partial least squares for on-line variable selection in   multivariate data streams|Brian McWilliams;Giovanni Montana|stat.ML|"In this paper we propose a computationally efficient algorithm for on-line variable selection in multivariate regression problems involving high dimensional data streams. The algorithm recursively extracts all the latent factors of a partial least squares solution and selects the most important variables for each factor. This is achieved by means of only one sparse singular value decomposition which can be efficiently updated on-line and in an adaptive fashion. Simulation results based on artificial data streams demonstrate that the algorithm is able to select important variables in dynamic settings where the correlation structure among the observed streams is governed by a few hidden components and the importance of each variable changes over time. We also report on an application of our algorithm to a multivariate version of the ""enhanced index tracking"" problem using financial data streams. The application consists of performing on-line asset allocation with the objective of overperforming two benchmark indices simultaneously."|2009-02-08T17:41:11Z|2009-02-08T17:41:11Z|stat
http://arxiv.org/abs/1009.2827v1|Fading of the X-ray flux from the black hole in the NGC 4472 globular   cluster RZ 2109|Thomas J. Maccarone;Arunav Kundu;Stephen E. Zepf;Katherine L. Rhode|astro-ph.HE|"We present the results of new X-ray observations of XMMU 122939.7+075333, the black hole (BH) in the globular cluster RZ 2109 in the Virgo Cluster galaxy NGC 4472. A combination of non-detections and marginal detections in several recent Swift and Chandra observations show that the source has varied by at least a factor of 20 in the past 6 years, and that the variations seem not just to be ""flickering."" This variation could be explained with changes in the absorption column intrinsic to the source no larger than those which were previously seen near the peak of the 1989 outburst of the Galactic BH X-ray binary V404 Cyg. The large amplitude variations are also a natural expectation from a hierarchical triple system with Kozai cycles -- the mechanism recently proposed to produce BH-white dwarf (WD) binaries in globular clusters. On the other hand, variation by such a large factor on timescales of years, rather than centuries, is very difficult to reconcile with the scenario in which the X-ray emission from XMMU 122939.7+075333 is due to fallback of material from a tidally destroyed or detonated WD."|2010-09-15T03:48:55Z|2010-09-15T03:48:55Z|astro-ph
http://arxiv.org/abs/1310.7522v2|Protrusion fluctuations direct cell motion|David Caballero;Raphael Voituriez;Daniel Riveline|q-bio.CB|Many physiological phenomena involve directional cell migration. It is usually attributed to chemical gradients in vivo. Recently, other cues have been shown to guide cells in vitro, including stiffness/adhesion gradients or micro-patterned adhesive motifs. However, the cellular mechanism leading to these biased migrations remains unknown, and, often, even the direction of motion is unpredictable. In this study, we show the key role of fluctuating protrusions on ratchet-like structures in driving NIH3T3 cell migration. We identified the concept of efficient protrusion and an associated direction index. Our analysis of the protrusion statistics facilitated the quantitative prediction of cell trajectories in all investigated conditions. We varied the external cues by changing the adhesive patterns. We also modified the internal cues using drug treatments, which modified the protrusion activity. Stochasticity affects the short- and long-term steps. We developed a theoretical model showing that an asymmetry in the protrusion fluctuations is sufficient for predicting all measures associated with the long-term motion, which can be described as a biased persistent random walk.|2013-10-28T18:18:18Z|2014-07-18T17:10:02Z|q-bio
http://arxiv.org/abs/1912.04185v2|Not so fast: LB-1 is unlikely to contain a 70 $M_{\odot}$ black hole|Kareem El-Badry;Eliot Quataert|astro-ph.SR|"The recently discovered binary LB-1 has been reported to contain a $\sim$\,$70\,M_{\odot}$ black hole (BH). The evidence for the unprecedentedly high mass of the unseen companion comes from reported radial velocity (RV) variability of the H$\alpha$ emission line, which has been proposed to originate from an accretion disk around a BH. We show that there is in fact no evidence for RV variability of the H$\alpha$ emission line, and that its apparent shifts instead originate from shifts in the luminous star's H$\alpha$ absorption line. If not accounted for, such shifts will cause a stationary emission line to appear to shift in anti-phase with the luminous star. We show that once the template spectrum of a B star is subtracted from the observed Keck/HIRES spectra of LB-1, evidence for RV variability vanishes. Indeed, the data rule out periodic variability of the line with velocity semi-amplitude $K_{\rm H\alpha} > 1.3\,\rm km\,s^{-1}$. This strongly suggests that the observed H$\alpha$ emission does not originate primarily from an accretion disk around a BH, and thus that the mass ratio cannot be constrained from the relative velocity amplitudes of the emission and absorption lines. The nature of the unseen companion remains uncertain, but a ""normal"" stellar-mass BH with mass $5\lesssim M/M_{\odot}\lesssim 20 $ seems most plausible. The H$\alpha$ emission likely originates primarily from circumbinary material, not from either component of the binary."|2019-12-09T17:05:08Z|2020-01-03T16:40:57Z|astro-ph
http://arxiv.org/abs/1706.08479v2|A Partial Solution to Continuous Blotto|Kostyantyn Mazur|q-fin.EC|This paper analyzes the structure of mixed-strategy equilibria for Colonel Blotto games, where the outcome on each battlefield is a polynomial function of the difference between the two players' allocations. This paper severely reduces the set of strategies that needs to be searched to find a Nash equilibrium. It finds that there exists a Nash equilibrium where both players' mixed strategies are discrete distributions, and it places an upper bound on the number of points in the supports of these discrete distributions.|2017-06-07T22:08:05Z|2017-09-14T04:55:18Z|q-fin
http://arxiv.org/abs/1508.00459v1|Unsupervised Learning in Genome Informatics|Ka-Chun Wong;Yue Li;Zhaolei Zhang|q-bio.GN|With different genomes available, unsupervised learning algorithms are essential in learning genome-wide biological insights. Especially, the functional characterization of different genomes is essential for us to understand lives. In this book chapter, we review the state-of-the-art unsupervised learning algorithms for genome informatics from DNA to MicroRNA.   DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant fraction of DNA regions (transcription factor binding sites) are bound by proteins (transcription factors) to regulate gene expression at different development stages in different tissues. To fully understand genetics, it is necessary of us to apply unsupervised learning algorithms to learn and infer those DNA regions. Here we review several unsupervised learning methods for deciphering the genome-wide patterns of those DNA regions.   MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic acid) species, regulate gene expression post-transcriptionally by forming imperfect base-pair with the target sites primarily at the 3$'$ untranslated regions of the messenger RNAs. Since the 1993 discovery of the first miRNA \emph{let-7} in worms, a vast amount of studies have been dedicated to functionally characterizing the functional impacts of miRNA in a network context to understand complex diseases such as cancer. Here we review several representative unsupervised learning frameworks on inferring miRNA regulatory network by exploiting the static sequence-based information pertinent to the prior knowledge of miRNA targeting and the dynamic information of miRNA activities implicated by the recently available large data compendia, which interrogate genome-wide expression profiles of miRNAs and/or mRNAs across various cell conditions.|2015-08-03T15:52:38Z|2015-08-03T15:52:38Z|q-bio
http://arxiv.org/abs/1807.09660v1|Hospitality Students' Perceptions towards Working in Hotels: a case   study of the faculty of tourism and hotels in Alexandria University|Sayed El-Houshy|econ.GN|The tourism and hospitality industry worldwide has been confronted with the problem of attracting and retaining quality employees. If today's students are to become the effective practitioners of tomorrow, it is fundamental to understand their perceptions of tourism employment. Therefore, this research aims at investigating the perceptions of hospitality students at the Faculty of Tourism in Alexandria University towards the industry as a career choice. A self-administrated questionnaire was developed to rate the importance of 20 factors in influencing career choice, and the extent to which hospitality as a career offers these factors. From the results, it is clear that students generally do not believe that the hospitality career will offer them the factors they found important. However, most of respondents (70.6%) indicated that they would work in the industry after graduation. Finally, a set of specific remedial actions that hospitality stakeholders could initiate to improve the perceptions of hospitality career are discussed.|2018-07-22T12:17:00Z|2018-07-22T12:17:00Z|econ
http://arxiv.org/abs/1905.10330v1|Dirac Delta Regression: Conditional Density Estimation with Clinical   Trials|Eric V. Strobl;Shyam Visweswaran|stat.ML|Personalized medicine seeks to identify the causal effect of treatment for a particular patient as opposed to a clinical population at large. Most investigators estimate such personalized treatment effects by regressing the outcome of a randomized clinical trial (RCT) on patient covariates. The realized value of the outcome may however lie far from the conditional expectation. We therefore introduce a method called Dirac Delta Regression (DDR) that estimates the entire conditional density from RCT data in order to visualize the probabilities across all possible treatment outcomes. DDR transforms the outcome into a set of asymptotically Dirac delta distributions and then estimates the density using non-linear regression. The algorithm can identify significant patient-specific treatment effects even when no population level effect exists. Moreover, DDR outperforms state-of-the-art algorithms in conditional density estimation on average regardless of the need for causal inference.|2019-05-24T16:42:56Z|2019-05-24T16:42:56Z|stat
http://arxiv.org/abs/1206.6979v1|The supernovae associated with gamma-ray bursts|David Bersier|astro-ph.HE|The connection between long GRBs and supernovae is now well established. I briefly review the evidence in favor of this connection and summarise where we are observationally. I also use a few events to exemplify what should be done and what type of data are needed. I also look at what we can learn from looking at SNe not associated with GRBs and see how GRBs fit into the broad picture of stellar explosions.|2012-06-29T10:36:07Z|2012-06-29T10:36:07Z|astro-ph
http://arxiv.org/abs/1507.06379v1|On the distribution and swim pressure of run-and-tumble particles in   confinement|Barath Ezhilan;Roberto Alonso-Matilla;David Saintillan|physics.flu-dyn|The spatial and orientational distribution in a dilute active suspension of non-Brownian run-and-tumble spherical swimmers confined between two planar hard walls is calculated theoretically. Using a kinetic model based on coupled bulk/surface probability density functions, we demonstrate the existence of a concentration wall boundary layer with thickness scaling with the run length, the absence of polarization throughout the channel, and the presence of sharp discontinuities in the bulk orientation distribution in the neighborhood of orientations parallel to the wall in the near-wall region. Our model is also applied to calculate the swim pressure in the system, which approaches the previously proposed ideal-gas behavior in wide channels but is found to decrease in narrow channels as a result of confinement. Monte-Carlo simulations are also performed for validation and show excellent quantitative agreement with our theoretical predictions.|2015-07-23T03:51:03Z|2015-07-23T03:51:03Z|physics
http://arxiv.org/abs/1809.06824v2|Matching in Dynamic Imbalanced Markets|Itai Ashlagi;Afshin Nikzad;Philipp Strack|econ.TH|We study dynamic matching in exchange markets with easy- and hard-to-match agents. A greedy policy, which attempts to match agents upon arrival, ignores the positive externality that waiting agents generate by facilitating future matchings. We prove that this trade-off between a ``thicker'' market and faster matching vanishes in large markets; A greedy policy leads to shorter waiting times, and more agents matched than any other policy. We empirically confirm these findings in data from the National Kidney Registry. Greedy matching achieves as many transplants as commonly-used policies (1.6\% more than monthly-batching), and shorter patient waiting times.|2018-09-18T16:45:09Z|2019-06-13T16:09:41Z|econ
http://arxiv.org/abs/1911.13063v1|Semiparametric Quantile Models for Ascending Auctions with Asymmetric   Bidders|Jayeeta Bhattacharya;Nathalie Gimenes;Emmanuel Guerre|econ.EM|The paper proposes a parsimonious and flexible semiparametric quantile regression specification for asymmetric bidders within the independent private value framework. Asymmetry is parameterized using powers of a parent private value distribution, which is generated by a quantile regression specification. As noted in Cantillon (2008), this covers and extends models used for efficient collusion, joint bidding and mergers among homogeneous bidders. The specification can be estimated for ascending auctions using the winning bids and the winner's identity. The estimation is two stage. The asymmetry parameters are estimated from the winner's identity using a simple maximum likelihood procedure. The parent quantile regression specification can be estimated using simple modifications of Gimenes (2017). A timber application reveals that weaker bidders have 30\% less chances to win the auction than stronger ones. It is also found that increasing participation in an asymmetric ascending auction may not be as beneficial as using an optimal reserve price as would have been expected from a result of Bulow and Klemperer (1996) valid under symmetry.|2019-11-29T11:24:48Z|2019-11-29T11:24:48Z|econ
http://arxiv.org/abs/1308.4951v1|Genome wide signals of pervasive positive selection in human evolution|David Enard;Philipp W. Messer;Dmitri Petrov|q-bio.PE|The role of positive selection in human evolution remains controversial. On the one hand, scans for positive selection have identified hundreds of candidate loci and the genome-wide patterns of polymorphism show signatures consistent with frequent positive selection. On the other hand, recent studies have argued that many of the candidate loci are false positives and that most apparent genome-wide signatures of adaptation are in fact due to reduction of neutral diversity by linked recurrent deleterious mutations, known as background selection. Here we analyze human polymorphism data from the 1,000 Genomes project (Abecasis et al. 2012) and detect signatures of pervasive positive selection once we correct for the effects of background selection. We show that levels of neutral polymorphism are lower near amino acid substitutions, with the strongest reduction observed specifically near functionally consequential amino acid substitutions. Furthermore, amino acid substitutions are associated with signatures of recent adaptation that should not be generated by background selection, such as the presence of unusually long and frequent haplotypes and specific distortions in the site frequency spectrum. We use forward simulations to show that the observed signatures require a high rate of strongly adaptive substitutions in the vicinity of the amino acid changes. We further demonstrate that the observed signatures of positive selection correlate more strongly with the presence of regulatory sequences, as predicted by ENCODE (Gerstein et al. 2012), than the positions of amino acid substitutions. Our results establish that adaptation was frequent in human evolution and provide support for the hypothesis of King and Wilson (King and Wilson 1975) that adaptive divergence is primarily driven by regulatory changes.|2013-08-22T18:38:52Z|2013-08-22T18:38:52Z|q-bio
http://arxiv.org/abs/1603.06727v1|Derivatives of isotropic positive definite functions on spheres|Mara Trübner;Johanna F. Ziegel|math.ST|We show that isotropic positive definite functions on the $d$-dimensional sphere which are $2k$ times differentiable at zero have $2k+[(d-1)/2]$ continuous derivatives on $(0,\pi)$. This result is analogous to the result for radial positive definite functions on Euclidean spaces. We prove optimality of the result for all odd dimensions. The proof relies on mont\'ee, descente and turning bands operators on spheres which parallel the corresponding operators originating in the work of Matheron for radial positive definite functions on Euclidian spaces.|2016-03-22T10:35:14Z|2016-03-22T10:35:14Z|math
http://arxiv.org/abs/1903.10965v1|Improving the Scalability of a Prosumer Cooperative Game with K-Means   Clustering|Liyang Han;Thomas Morstyn;Constance Crozier;Malcolm McCulloch|cs.CE|"Among the various market structures under peer-to-peer energy sharing, one model based on cooperative game theory provides clear incentives for prosumers to collaboratively schedule their energy resources. The computational complexity of this model, however, increases exponentially with the number of participants. To address this issue, this paper proposes the application of K-means clustering to the energy profiles following the grand coalition optimization. The cooperative model is run with the ""clustered players"" to compute their payoff allocations, which are then further distributed among the prosumers within each cluster. Case studies show that the proposed method can significantly improve the scalability of the cooperative scheme while maintaining a high level of financial incentives for the prosumers."|2019-03-26T15:44:16Z|2019-03-26T15:44:16Z|cs
http://arxiv.org/abs/1507.06370v2|Sum-of-Squares Lower Bounds for Sparse PCA|Tengyu Ma;Avi Wigderson|cs.LG|"This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the {\em Sparse Principal Component Analysis} (Sparse PCA) problem, and the family of {\em Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\em in principle} detected using only $n \approx k\log p$ (Gaussian or Bernoulli) samples, but all {\em efficient} (polynomial time) algorithms known require $n \approx k^2$ samples. It was also known that this quadratic gap cannot be improved by the the most basic {\em semi-definite} (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or ""pseudo-expectations"") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem."|2015-07-23T01:50:43Z|2015-10-18T05:50:16Z|cs
http://arxiv.org/abs/1807.07360v1|Green function of a random walk in a cone|Jetlir Duraj;Vitali Wachtel|math.PR|This paper studies the asymptotic behavior of the Green function of a multidimensional random walk killed when leaving a convex cone with smooth boundary. Our results imply uniqueness, up to a multiplicative factor, of the positive harmonic function for the killed random walk.|2018-07-19T12:13:49Z|2018-07-19T12:13:49Z|math
http://arxiv.org/abs/1703.09283v1|Model atmospheres of sub-stellar mass objects|Ivan Hubeny|astro-ph.SR|We present an outline of basic assumptions and governing structural equations describing atmospheres of substellar mass objects, in particular the extrasolar giant planets and brown dwarfs. Although most of the presentation of the physical and numerical background is generic, details of the implementation pertain mostly to the code CoolTlusty. We also present a review of numerical approaches and computer codes devised to solve the structural equations, and make a critical evaluation of their efficiency and accuracy.|2017-03-27T19:43:16Z|2017-03-27T19:43:16Z|astro-ph
http://arxiv.org/abs/1907.01049v1|Permutation inference with a finite number of heterogeneous clusters|Andreas Hagemann|econ.EM|I introduce a simple permutation procedure to test conventional (non-sharp) hypotheses about the effect of a binary treatment in the presence of a finite number of large, heterogeneous clusters when the treatment effect is identified by comparisons across clusters. The procedure asymptotically controls size by applying a level-adjusted permutation test to a suitable statistic. The adjustments needed for most empirically relevant situations are tabulated in the paper. The adjusted permutation test is easy to implement in practice and performs well at conventional levels of significance with at least four treated clusters and a similar number of control clusters. It is particularly robust to situations where some clusters are much more variable than others. Examples and an empirical application are provided.|2019-07-01T20:15:04Z|2019-07-01T20:15:04Z|econ
http://arxiv.org/abs/1108.4879v1|Using Supervised Learning to Improve Monte Carlo Integral Estimation|Brendan Tracey;David Wolpert;Juan J. Alonso|stat.ML|Monte Carlo (MC) techniques are often used to estimate integrals of a multivariate function using randomly generated samples of the function. In light of the increasing interest in uncertainty quantification and robust design applications in aerospace engineering, the calculation of expected values of such functions (e.g. performance measures) becomes important. However, MC techniques often suffer from high variance and slow convergence as the number of samples increases. In this paper we present Stacked Monte Carlo (StackMC), a new method for post-processing an existing set of MC samples to improve the associated integral estimate. StackMC is based on the supervised learning techniques of fitting functions and cross validation. It should reduce the variance of any type of Monte Carlo integral estimate (simple sampling, importance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We report on an extensive set of experiments confirming that the StackMC estimate of an integral is more accurate than both the associated unprocessed Monte Carlo estimate and an estimate based on a functional fit to the MC samples. These experiments run over a wide variety of integration spaces, numbers of sample points, dimensions, and fitting functions. In particular, we apply StackMC in estimating the expected value of the fuel burn metric of future commercial aircraft and in estimating sonic boom loudness measures. We compare the efficiency of StackMC with that of more standard methods and show that for negligible additional computational cost significant increases in accuracy are gained.|2011-08-24T16:22:55Z|2011-08-24T16:22:55Z|stat
http://arxiv.org/abs/1809.03834v1|House Price Modeling with Digital Census|Enwei Zhu;Stanislav Sobolevsky|econ.EM|"Urban house prices are strongly associated with local socioeconomic factors. In literature, house price modeling is based on socioeconomic variables from traditional census, which is not real-time, dynamic and comprehensive. Inspired by the emerging concept of ""digital census"" - using large-scale digital records of human activities to measure urban population dynamics and socioeconomic conditions, we introduce three typical datasets, namely 311 complaints, crime complaints and taxi trips, into house price modeling. Based on the individual housing sales data in New York City, we provide comprehensive evidence that these digital census datasets can substantially improve the modeling performances on both house price levels and changes, regardless whether traditional census is included or not. Hence, digital census can serve as both effective alternatives and complements to traditional census for house price modeling."|2018-08-29T16:13:58Z|2018-08-29T16:13:58Z|econ
http://arxiv.org/abs/1904.09195v2|Smoothing of the slowly extracted coasting beam from a synchrotron|Rahul Singh;Peter Forck;Stefan Sorge|physics.acc-ph|Slow extraction of beam from synchrotrons or storage rings as required by many fixed target experiments is performed by controlled excitation and feeding of a structural lattice resonance. Due to the sensitive nature of this resonant extraction process, the temporal structure of the extracted beam is modulated by the minuscule current fluctuations present on the quadrupole magnet power supplies. Such a modulation lead to pile-ups in detectors and a significant reduction in accumulated event statistics. This contribution proposes and experimentally demonstrates that by an introduction of further modulation on quadrupole currents with a specific amplitude and frequency, the inherent power supply fluctuations are mitigated leading to a smoothening of the beam temporal structure. The slow extraction beam dynamics associated with this method are explained along with the operational results.|2019-04-19T13:30:28Z|2019-07-31T11:12:23Z|physics
http://arxiv.org/abs/1403.4291v3|An importance sampling approach for copula models in insurance|Philipp Arbenz;Mathieu Cambou;Marius Hofert|stat.CO|An importance sampling approach for sampling copula models is introduced. We propose two algorithms that improve Monte Carlo estimators when the functional of interest depends mainly on the behaviour of the underlying random vector when at least one of the components is large. Such problems often arise from dependence models in finance and insurance. The importance sampling framework we propose is general and can be easily implemented for all classes of copula models from which sampling is feasible. We show how the proposal distribution of the two algorithms can be optimized to reduce the sampling error. In a case study inspired by a typical multivariate insurance application, we obtain variance reduction factors between 10 and 30 in comparison to standard Monte Carlo estimators.|2014-03-17T22:28:14Z|2015-04-07T17:57:36Z|stat
http://arxiv.org/abs/1908.06602v1|Beta-Binomial stick-breaking non-parametric prior|María F. Gil-Leyva;Ramsés H. Mena;Theodoros Nicoleris|math.ST|A new class of nonparametric prior distributions, termed Beta-Binomial stick-breaking process, is proposed. By allowing the underlying length random variables to be dependent through a Beta marginals Markov chain, an appealing discrete random probability measure arises. The chain's dependence parameter controls the ordering of the stick-breaking weights, and thus tunes the model's label-switching ability. Also, by tuning this parameter, the resulting class contains the Dirichlet process and the Geometric process priors as particular cases, which is of interest for fast convergence of MCMC implementations.   Some properties of the model are discussed and a density estimation algorithm is proposed and tested with simulated datasets.|2019-08-19T05:48:11Z|2019-08-19T05:48:11Z|math
http://arxiv.org/abs/1005.1862v4|On the estimation of integrated covariance matrices of high dimensional   diffusion processes|Xinghua Zheng;Yingying Li|stat.ME|We consider the estimation of integrated covariance (ICV) matrices of high dimensional diffusion processes based on high frequency observations. We start by studying the most commonly used estimator, the realized covariance (RCV) matrix. We show that in the high dimensional case when the dimension $p$ and the observation frequency $n$ grow in the same rate, the limiting spectral distribution (LSD) of RCV depends on the covolatility process not only through the targeting ICV, but also on how the covolatility process varies in time. We establish a Mar\v{c}enko--Pastur type theorem for weighted sample covariance matrices, based on which we obtain a Mar\v{c}enko--Pastur type theorem for RCV for a class $\mathcal{C}$ of diffusion processes. The results explicitly demonstrate how the time variability of the covolatility process affects the LSD of RCV. We further propose an alternative estimator, the time-variation adjusted realized covariance (TVARCV) matrix. We show that for processes in class $\mathcal {C}$, the TVARCV possesses the desirable property that its LSD depends solely on that of the targeting ICV through the Mar\v{c}enko--Pastur equation, and hence, in particular, the TVARCV can be used to recover the empirical spectral distribution of the ICV by using existing algorithms.|2010-05-11T15:29:29Z|2012-03-13T08:38:31Z|stat
http://arxiv.org/abs/math/0602451v1|Optimal consumption in discrete-time financial models with industrial   investment opportunities and nonlinear returns|Bruno Bouchard;Huyên Pham|math.PR|We consider a general discrete-time financial market with proportional transaction costs as in [Kabanov, Stricker and R\'{a}sonyi Finance and Stochastics 7 (2003) 403--411] and [Schachermayer Math. Finance 14 (2004) 19--48]. In addition to the usual investment in financial assets, we assume that the agents can invest part of their wealth in industrial projects that yield a nonlinear random return. We study the problem of maximizing the utility of consumption on a finite time period. The main difficulty comes from the nonlinearity of the nonfinancial assets' return. Our main result is to show that existence holds in the utility maximization problem. As an intermediary step, we prove the closedness of the set $A_T$ of attainable claims under a robust no-arbitrage property similar to the one introduced in [Schachermayer Math. Finance 14 (2004) 19--48] and further discussed in [Kabanov, Stricker and R\'{a}sonyi Finance and Stochastics 7 (2003) 403--411]. This allows us to provide a dual formulation for $A_T$.|2006-02-21T09:22:28Z|2006-02-21T09:22:28Z|math
http://arxiv.org/abs/1203.1450v2|AD in Fortran, Part 2: Implementation via Prepreprocessor|Alexey Radul;Barak A. Pearlmutter;Jeffrey Mark Siskind|cs.PL|"We describe an implementation of the Farfel Fortran AD extensions. These extensions integrate forward and reverse AD directly into the programming model, with attendant benefits to flexibility, modularity, and ease of use. The implementation we describe is a ""prepreprocessor"" that generates input to existing Fortran-based AD tools. In essence, blocks of code which are targeted for AD by Farfel constructs are put into subprograms which capture their lexical variable context, and these are closure-converted into top-level subprograms and specialized to eliminate EXTERNAL arguments, rendering them amenable to existing AD preprocessors, which are then invoked, possibly repeatedly if the AD is nested."|2012-03-07T12:16:30Z|2012-03-08T09:56:48Z|cs
http://arxiv.org/abs/1909.07380v2|How far can we push deconvolution? A SCUBA-2 test case|Stephen Serjeant|astro-ph.GA|How far can we use multi-wavelength cross-identifications to deconvolve far-infrared images? In this short research note I explore a test case of CLEAN deconvolutions of simulated confused 850 micron SCUBA-2 data, and explore the possible scientific applications of combining this data with ostensibly deeper TolTEC Large Scale Structure (LSS) survey 1.1mm-2mm data. I show that the SCUBA-2 can be reconstructed to the 1.1mm LMT resolution and achieve an 850 micron deconvolved sensitivity of 0.7 mJy RMS, an improvement of at least ~1:5x over naive point source filtered images. The TolTEC/SCUBA-2 combination can constrain cold (<10K) observed-frame colour temperatures, where TolTEC alone cannot.|2019-09-16T15:35:02Z|2019-09-20T10:01:39Z|astro-ph
http://arxiv.org/abs/1206.4306v3|Star-Galaxy Classification in Multi-Band Optical Imaging|Ross Fadely;David W. Hogg;Beth Willman|astro-ph.IM|Ground-based optical surveys such as PanSTARRS, DES, and LSST, will produce large catalogs to limiting magnitudes of r > 24. Star-galaxy separation poses a major challenge to such surveys because galaxies---even very compact galaxies---outnumber halo stars at these depths. We investigate photometric classification techniques on stars and galaxies with intrinsic FWHM < 0.2 arcsec. We consider unsupervised spectral energy distribution template fitting and supervised, data-driven Support Vector Machines (SVM). For template fitting, we use a Maximum Likelihood (ML) method and a new Hierarchical Bayesian (HB) method, which learns the prior distribution of template probabilities from the data. SVM requires training data to classify unknown sources; ML and HB don't. We consider i.) a best-case scenario (SVM_best) where the training data is (unrealistically) a random sampling of the data in both signal-to-noise and demographics, and ii.) a more realistic scenario where training is done on higher signal-to-noise data (SVM_real) at brighter apparent magnitudes. Testing with COSMOS ugriz data we find that HB outperforms ML, delivering ~80% completeness, with purity of ~60-90% for both stars and galaxies, respectively. We find no algorithm delivers perfect performance, and that studies of metal-poor main-sequence turnoff stars may be challenged by poor star-galaxy separation. Using the Receiver Operating Characteristic curve, we find a best-to-worst ranking of SVM_best, HB, ML, and SVM_real. We conclude, therefore, that a well trained SVM will outperform template-fitting methods. However, a normally trained SVM performs worse. Thus, Hierarchical Bayesian template fitting may prove to be the optimal classification method in future surveys.|2012-06-19T20:00:00Z|2012-10-22T19:34:11Z|astro-ph
http://arxiv.org/abs/1311.7403v1|Remarks on Privileged Words|Michael Forsyth;Amlesh Jayakumar;Jeffrey Shallit|cs.FL|We discuss the notion of privileged word, recently introduced by Peltomaki. A word w is privileged if it is of length <=1, or has a privileged border that occurs exactly twice in w. We prove the following results: (1) if w^k is privileged for some k >=1, then w^j is privileged for all j >= 0; (2) the language of privileged words is neither regular nor context-free; (3) there is a linear-time algorithm to check if a given word is privileged; and (4) there are at least 2^{n-5}/n^2 privileged binary words of length n.|2013-11-28T19:28:37Z|2013-11-28T19:28:37Z|cs
http://arxiv.org/abs/1302.3966v3|Effect of Temperature Wave on Diffusive Transport of Weakly-Soluble   Substances in Liquid-Saturated Porous Media|Pavel V. Krauzin;Denis S. Goldobin|physics.geo-ph|We study the effect of surface temperature oscillations on diffusive transport of solutes of weaklysoluble substances through liquid-saturated porous media. Temperature wave induced by these oscillations and decaying deep in the porous massif creates the solubility wave along with the corresponding solute diffusion flux wave. When the non-dissolved fraction is immobilized in pores---for gases the bubbles can be immobilized by the surface tension force, for solids (e.g., limestone, gas-hydrates) the immobilization of non-dissolved phase is obvious---the only remaining mechanisms of mass transport are related to solute flux through liquid in pores. We evaluate analytically the generated time-average mass flux for the case of medium everywhere littered with non-dissolved phase and reveal the significant effect of the temperature wave on the substance release from the massif and non-dissolved mass redistribution within the massif. Analytical theory is validated with numerical calculations.|2013-02-16T13:16:01Z|2014-09-10T10:01:16Z|physics
http://arxiv.org/abs/1605.05798v2|MCMC for Imbalanced Categorical Data|James E. Johndrow;Aaron Smith;Natesh Pillai;David B. Dunson|math.ST|Many modern applications collect highly imbalanced categorical data, with some categories relatively rare. Bayesian hierarchical models combat data sparsity by borrowing information, while also quantifying uncertainty. However, posterior computation presents a fundamental barrier to routine use; a single class of algorithms does not work well in all settings and practitioners waste time trying different types of MCMC approaches. This article was motivated by an application to quantitative advertising in which we encountered extremely poor computational performance for common data augmentation MCMC algorithms but obtained excellent performance for adaptive Metropolis. To obtain a deeper understanding of this behavior, we give strong theory results on computational complexity in an infinitely imbalanced asymptotic regime. Our results show computational complexity of Metropolis is logarithmic in sample size, while data augmentation is polynomial in sample size. The root cause of poor performance of data augmentation is a discrepancy between the rates at which the target density and MCMC step sizes concentrate. In general, MCMC algorithms that have a similar discrepancy will fail in large samples - a result with substantial practical impact.|2016-05-19T02:45:46Z|2017-06-26T15:06:27Z|math
http://arxiv.org/abs/1301.4207v2|Anticipatory Systems, Preferences, Averages: Inflation, Uncertain   Phenomena, Management|Leonid A. Shapiro|q-fin.GN|"Behavior of systems that are functions of anticipated behavior of other systems, whose own behavior is also anticipatory but homeostatic and determined by hierarchical ordering, which changes over time, of sets of possible environments that are not co-possible, is proven to be highly non-linear and sensitively dependent on precise parameters. Averages and other kinds of aggregates cannot be calculated for sets of measurements of behavior of systems, defined in this essay, that are ""index complex"" in this way. This includes many systems, for instance, social behavior, where anticipation of behavior of other individuals plays a central role. Anticipation of preferences of economic actors are discussed in this way. Analysis by way of generalized functions of complex variables is done for these kinds of systems, and equations of change of state are formally described. Behavior that comprises of responses to market interest rates is taken for example. Continuity assumptions in economics analyzed in this context. Anticipatory responses to inflation in economics are discussed. Applications to theory of production are presented."|2013-01-17T20:22:17Z|2013-07-16T07:07:52Z|q-fin
http://arxiv.org/abs/1803.11253v1|A comprehensive review of Chronic Kidney Disease of Unknown Etiology|Adoni Fernando;Nivethika Sivakumaran|q-bio.QM|This review was written to provide a comprehensive summary of the suggested etiologies of Chronic Kidney Disease of Unknown Etiology (CKDu) in Sri Lanka. In this review, Chronic Kidney Disease (CKD) is explained in detail and its known etiologies are discussed. CKDu is defined and its epidemiology is discussed, with the compilation of statistic from over 15 research papers through the years 2000 to present.|2018-03-28T03:55:06Z|2018-03-28T03:55:06Z|q-bio
http://arxiv.org/abs/math/0505218v1|Convergence in capacity|Urban Cegrell|math.CV|The purpose of this paper is to study convergence of Monge-Ampere measures associated to sequences of plurisubharmonic functions defined on a hyperconvex subset of ${\mathbb C^n}$.|2005-05-11T14:48:39Z|2005-05-11T14:48:39Z|math
http://arxiv.org/abs/1211.1300v1|An Accurate Flux Density Scale from 1 to 50 GHz|Rick A. Perley;Bryan J. Butler|astro-ph.IM|We develop an absolute flux density scale for cm-wavelength astronomy by combining accurate flux density ratios determined by the VLA between the planet Mars and a set of potential calibrators with the Rudy thermophysical emission model of Mars, adjusted to the absolute scale established by WMAP. The radio sources 3C123, 3C196, 3C286 and 3C295 are found to be varying at a level of less than ~5% per century at all frequencies between 1 and 50 GHz, and hence are suitable as flux density standards. We present polynomial expressions for their spectral flux densities, valid from 1 to 50 GHz, with absolute accuracy estimated at 1-3% depending on frequency. Of the four sources, 3C286 is the most compact and has the flattest spectral index, making it the most suitable object on which to establish the spectral flux density scale. The sources 3C48, 3C138, 3C147, NGC7027, NGC6542, and MWC349 show significant variability on various timescales. Polynomial coefficients for the spectral flux density are developed for 3C48, 3C138, and 3C147 for each of the seventeen observation dates, spanning 1983 through 2012. The planets Venus, Uranus, and Neptune are included in our observations, and we derive their brightness temperatures over the same frequency range.|2012-11-06T16:37:31Z|2012-11-06T16:37:31Z|astro-ph
http://arxiv.org/abs/1708.04786v1|Coherent Perfect Absorption: an electromagnetic perspective|Sanjeeb Dey;Suneel Singh|physics.optics|We present a simple closed expression for determining the condition for Coherent Perfect Absorption derived through electromagnetic wave treatment and interface boundary conditions. Apart from providing physical insight this expression can be used to estimate the values of various parameters required for observation of coherent perfect absorption in a given medium characterized by a complex dielectric constant. The results of the theoretical expression are found to be in good agreement with those obtained through numerical simulations.|2017-08-16T06:42:22Z|2017-08-16T06:42:22Z|physics
http://arxiv.org/abs/0908.2378v1|Long-range energy transfer in proteins|Francesco Piazza;Yves-Henri Sanejouand|q-bio.BM|Proteins are large and complex molecular machines. In order to perform their function, most of them need energy, e.g. either in the form of a photon, like in the case of the visual pigment rhodopsin, or through the breaking of a chemical bond, as in the presence of adenosine triphosphate (ATP). Such energy, in turn, has to be transmitted to specific locations, often several tens of Angstroms away from where it is initially released. Here we show, within the framework of a coarse-grained nonlinear network model, that energy in a protein can jump from site to site with high yields, covering in many instances remarkably large distances. Following single-site excitations, few specific sites are targeted, systematically within the stiffest regions. Such energy transfers mark the spontaneous formation of a localized mode of nonlinear origin at the destination site, which acts as an efficient energy-accumulating centre. Interestingly, yields are found to be optimum for excitation energies in the range of biologically relevant ones.|2009-08-17T15:59:53Z|2009-08-17T15:59:53Z|q-bio
http://arxiv.org/abs/2001.02146v1|A DNA damage multi-scale model for NTCP in proton and hadron therapy|Ramin Abolfath;Chris Peeler;Dragan Mirkovic;Radhe Mohan;David Grosshans|physics.med-ph|{\bf Purpose}: To develop a first principle and multi-scale model for normal tissue complication probability (NTCP) as a function of dose and LET for proton and in general for particle therapy with a goal of incorporating nano-scale radio-chemical to macro-scale cell biological pathways, spanning from initial DNA damage to tissue late effects.   {\bf Methods}: The method is combination of analytical and multi-scale computational steps including (1) derivation of functional dependencies of NTCP on DNA driven cell lethality in nanometer and mapping to dose and LET in millimeter, and (2) 3D-surface fitting to Monte Carlo data set generated based on post radiation image change and gathered for a cohort of 14 pediatric patients treated by scanning beam of protons for ependymoma. We categorize voxel-based dose and LET associated with development of necrosis in NTCP.   {\bf Result}: Our model fits well the clinical data, generated for post radiation tissue toxicity and necrosis. The fitting procedure results in extraction of in-{\it vivo} radio-biological $\alpha$-$\beta$ indices and their numerical values.   {\bf Discussion and conclusion}: The NTCP model, explored in this work, allows to correlate the tissue toxicities to DNA initial damage, cell lethality and the properties and qualities of radiation, dose and LET.|2020-01-07T16:10:08Z|2020-01-07T16:10:08Z|physics
http://arxiv.org/abs/1311.4290v1|Study on Antibody-Virus Interaction using Molecular Dynamics: Two   Dimensional Simulation on Immunoglobulin Reaction against Human   Papillomavirus|Luman Haris;Sony Suhandono;Siti Nurul Khotimah;Freddy Haryanto;Sparisoma Viridi|q-bio.CB|Human Papillomavirus (HPV) has been known as one of the cause of virus-induced cancer such as cervical cancer and carcinoma. Among other types of cancer, this type has higher chance in being prevented earlier. The main idea is to eradicate the virus as soon as it enters the body by marking it with antibodies; signaling the immune system to dispose of it. However, the antibodies must be trained to recognize the virus. They can be trained by inserting an object similar to the virus allowing them to learn to recognize and surround the inserted object. In response to this, molecular dynamics simulation was chosen to study the antibody-virus interaction. In this work, two-dimensional case that involves HPV and immunoglobulin (Ig) was studied and observed. Two types of objects will be defined; one stands for HPV while another stands for antibodies. The interaction between the two objects will be governed by two forces; Coulomb force and repulsive contact force. Through the definition of some rules and condition, the antibodies' motion was observed. The influence of antibody concentration, and the antibody's type and their appearance sequence were observed both quantitatively and visually.   Keywords: antibody-virus interaction, molecular dynamics, HPV, immunoglobulin.|2013-11-18T08:25:02Z|2013-11-18T08:25:02Z|q-bio
http://arxiv.org/abs/1205.4708v2|Emergence of spatial spin-wave correlations in a cold atomic gas|Y. O. Dudin;F. Bariani;A. Kuzmich|physics.atom-ph|Rydberg spin waves are optically excited in a quasi-one-dimensional atomic sample of Rb atoms. Pair-wise spin-wave correlations are observed by a spatially selective transfer of the quantum state onto a light field and photoelectric correlation measurements of the light. The correlations are interpreted in terms of the dephasing of multiply-excited spin waves by long-range Rydberg interactions.|2012-05-21T19:50:27Z|2012-08-23T14:01:24Z|physics
http://arxiv.org/abs/1512.04573v1|Mathematical model of the interaction between baroreflex and cerebral   autoregulation|Adam Mahdi;Mette S. Olufsen;Stephen J. Payne|q-bio.TO|Baroreflex (BR) and cerebral autoregulation (CA) are two important mechanisms regulating blood pressure and flow. However, the functional relationship between BR and CA in humans is unknown. Since BR impairment is an adverse prognostic indicator for both cardiac and cerebrovascular diseases it would be of clinical interest to better understand the relationship between BR and CA. Motivated by this observation we develop a simple mathematical framework aiming to simulate the effects of BR on the cerebral blood flow dynamics.|2015-09-02T12:35:57Z|2015-09-02T12:35:57Z|q-bio
http://arxiv.org/abs/1710.02944v1|A Unified Approach on the Local Power of Panel Unit Root Tests|Zhongwen Liang|econ.EM|In this paper, a unified approach is proposed to derive the exact local asymptotic power for panel unit root tests, which is one of the most important issues in nonstationary panel data literature. Two most widely used panel unit root tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and Im-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically studied for various situations to illustrate our method. Our approach is characteristic function based, and can be used directly in deriving the moments of the asymptotic distributions of these test statistics under the null and the local-to-unity alternatives. For the LLC test, the approach provides an alternative way to obtain the results that can be derived by the existing method. For the IPS test, the new results are obtained, which fills the gap in the literature where few results exist, since the IPS test is non-admissible. Moreover, our approach has the advantage in deriving Edgeworth expansions of these tests, which are also given in the paper. The simulations are presented to illustrate our theoretical findings.|2017-10-09T05:57:06Z|2017-10-09T05:57:06Z|econ
http://arxiv.org/abs/q-bio/0402048v1|Reliably determining which genes have a high posterior probability of   differential expression: A microarray application of decision-theoretic   multiple testing|David R. Bickel|q-bio.QM|Microarray data are often used to determine which genes are differentially expressed between groups, for example, between treatment and control groups. There are methods of determining which genes have a high probability of differential expression, but those methods depend on the estimation of probability densities. Theoretical results have shown such estimation to be unreliable when high-probability genes are identified.   The genes that are probably differentially expressed can be found using decision theory instead of density estimation. Simulations show that the proposed decision-theoretic method is much more reliable than a density-estimation method. The proposed method is used to determine which genes to consider differentially expressed between patients with different types of cancer.   The proposed method determines which genes have a high probability of differential expression. It can be applied to data sets that have replicate microarrays in each of two or more groups of patients or experiments.|2004-02-29T14:56:23Z|2004-02-29T14:56:23Z|q-bio
http://arxiv.org/abs/1805.03721v3|Decentralized Collaborative Knowledge Management using Git|Natanael Arndt;Patrick Naumann;Norman Radtke;Michael Martin;Edgard Marx|cs.DB|The World Wide Web and the Semantic Web are designed as a network of distributed services and datasets. The distributed character of the Web brings manifold collaborative possibilities to interchange data. The commonly adopted collaborative solutions for RDF data are centralized (e.g. SPARQL endpoints and wiki systems). But to support distributed collaboration, a system is needed, that supports divergence of datasets, brings the possibility to conflate diverged states, and allows distributed datasets to be synchronized. In this paper, we present Quit Store, it was inspired by and it builds upon the successful Git system. The approach is based on a formal expression of evolution and consolidation of distributed datasets. During the collaborative curation process, the system automatically versions the RDF dataset and tracks provenance information. It also provides support to branch, merge, and synchronize distributed RDF datasets. The merging process is guarded by specific merge strategies for RDF data. Finally, we use our reference implementation to show overall good performance and demonstrate the practical usability of the system.|2018-05-09T20:24:20Z|2018-10-16T07:53:05Z|cs
http://arxiv.org/abs/1310.2076v1|An Investigation of Methods for Handling Missing Data with Penalized   Regression|Yunjin Choi;Robert Tibshirani|stat.AP|We investigate methods for penalized regression in the presence of missing observations. This paper introduces a method for estimating the parameters which compensates for the missing observations. We first, derive an unbiased estimator of the objective function with respect to the missing data and then, modify the criterion to ensure convexity. Finally, we extend our approach to a family of models that embraces the mean imputation method. These approaches are compared to the mean imputation method, one of the simplest methods for dealing with missing observations problem, via simulations. We also investigate the problem of making predictions when there are missing values in the test set.|2013-10-08T10:26:30Z|2013-10-08T10:26:30Z|stat
http://arxiv.org/abs/1809.08960v1|On a gap between rational annuitization price for producer and price for   customer|Nikolai Dokuchaev|econ.GN|"The paper studies pricing of insurance products focusing on the pricing of annuities under uncertainty. This pricing problem is crucial for financial decision making and was studied intensively, however, many open questions still remain. In particular, there is a so-called ""annuity puzzle"" related to certain inconsistency of existing financial theory with the empirical observations for the annuities market. The paper suggests a pricing method based on the risk minimization such that both producer and customer seek to minimize the mean square hedging error accepted as a measure of risk. This leads to two different versions of the pricing problem: the selection of the annuity price given the rate of regular payments, and the selection of the rate of payments given the annuity price. It appears that solutions of these two problems are different. This can contribute to explanation for the ""annuity puzzle""."|2018-09-24T14:16:48Z|2018-09-24T14:16:48Z|econ
http://arxiv.org/abs/1910.13480v1|Planetary Astronomy-Understanding the Origin of the Solar System|S. M. Lawler;A. C. Boley;M. Connors;W. Fraser;B. Gladman;C. L. Johnson;J. J. Kavelaars;G. Osinski;L. Philpott;J. Rowe;P. Wiegert;R. Winslow|astro-ph.EP|There is a vibrant and effective planetary science community in Canada. We do research in the areas of meteoritics, asteroid and trans-Neptunian object orbits and compositions, and space weather, and are involved in space probe missions to study planetary surfaces and interiors. For Canadian planetary scientists to deliver the highest scientific impact possible, we have several recommendations. Our top recommendation is to join LSST and gain access to the full data releases by hosting a data centre, which could be done by adding to the CADC, which is already highly involved in hosting planetary data and supporting computational modelling for orbital studies. We also support MSE, which can provide spectroscopy and thus compositional information for thousands of small bodies. We support a Canadian-led microsatellite, POEP, which will provide small body sizes by measuring occultations. We support the idea of piggybacking space weather instruments on other astronomical space probes to provide data for the space weather community. Many Canadian planetary scientists are involved in space probe missions, but through haphazard and temporary arrangements like co-appointments at US institutions, so we would like the community to support Canadian researchers to participate in these large, international missions.|2019-10-29T19:02:54Z|2019-10-29T19:02:54Z|astro-ph
http://arxiv.org/abs/1211.3004v2|From Rome to the Antipodes: the medieval form of the world|Amelia Carolina Sparavigna|physics.pop-ph|Here we discuss how some medieval scholars in the Western Europe viewed the form of the world and the problem of the Antipodes, starting from the Natural History written by Pliny and ending in the Hell of Dante Alighieri. From the center of the Earth, Dante and Virgil came to the Antipodes: eventually, their existence was accepted. Among the others, we will discuss the works of the Venerable Bede and Sylvester the Second.|2012-11-13T14:40:46Z|2012-11-14T18:33:00Z|physics
http://arxiv.org/abs/1802.09725v1|High-dimensional ABC|D. J. Nott;V. M. -H. Ong;Y. Fan;S. A. Sisson|stat.CO|"This Chapter, ""High-dimensional ABC"", is to appear in the forthcoming Handbook of Approximate Bayesian Computation (2018). It details the main ideas and concepts behind extending ABC methods to higher dimensions, with supporting examples and illustrations."|2018-02-27T05:47:13Z|2018-02-27T05:47:13Z|stat
http://arxiv.org/abs/1510.06112v1|Dimensionality Reduction for Binary Data through the Projection of   Natural Parameters|Andrew J. Landgraf;Yoonkyung Lee|stat.ML|Principal component analysis (PCA) for binary data, known as logistic PCA, has become a popular alternative to dimensionality reduction of binary data. It is motivated as an extension of ordinary PCA by means of a matrix factorization, akin to the singular value decomposition, that maximizes the Bernoulli log-likelihood. We propose a new formulation of logistic PCA which extends Pearson's formulation of a low dimensional data representation with minimum error to binary data. Our formulation does not require a matrix factorization, as previous methods do, but instead looks for projections of the natural parameters from the saturated model. Due to this difference, the number of parameters does not grow with the number of observations and the principal component scores on new data can be computed with simple matrix multiplication. We derive explicit solutions for data matrices of special structure and provide computationally efficient algorithms for solving for the principal component loadings. Through simulation experiments and an analysis of medical diagnoses data, we compare our formulation of logistic PCA to the previous formulation as well as ordinary PCA to demonstrate its benefits.|2015-10-21T02:25:33Z|2015-10-21T02:25:33Z|stat
http://arxiv.org/abs/1812.04486v1|Trade Selection with Supervised Learning and OCA|David Saltiel;Eric Benhamou|cs.LG|In recent years, state-of-the-art methods for supervised learning have exploited increasingly gradient boosting techniques, with mainstream efficient implementations such as xgboost or lightgbm. One of the key points in generating proficient methods is Feature Selection (FS). It consists in selecting the right valuable effective features. When facing hundreds of these features, it becomes critical to select best features. While filter and wrappers methods have come to some maturity, embedded methods are truly necessary to find the best features set as they are hybrid methods combining features filtering and wrapping. In this work, we tackle the problem of finding through machine learning best a priori trades from an algorithmic strategy. We derive this new method using coordinate ascent optimization and using block variables. We compare our method to Recursive Feature Elimination (RFE) and Binary Coordinate Ascent (BCA). We show on a real life example the capacity of this method to select good trades a priori. Not only this method outperforms the initial trading strategy as it avoids taking loosing trades, it also surpasses other method, having the smallest feature set and the highest score at the same time. The interest of this method goes beyond this simple trade classification problem as it is a very general method to determine the optimal feature set using some information about features relationship as well as using coordinate ascent optimization.|2018-12-09T21:07:06Z|2018-12-09T21:07:06Z|cs
http://arxiv.org/abs/1703.01137v3|Model Spaces for Risk Measures|Felix-Benedikt Liebrich;Gregor Svindland|q-fin.RM|We show how risk measures originally defined in a model free framework in terms of acceptance sets and reference assets imply a meaningful underlying probability structure. Hereafter we construct a maximal domain of definition of the risk measure respecting the underlying ambiguity profile. We particularly emphasise liquidity effects and discuss the correspondence between properties of the risk measure and the structure of this domain as well as subdifferentiability properties.   Keywords: Model free risk assessment, extension of risk measures, continuity properties of risk measures, subgradients.|2017-03-03T12:55:15Z|2017-11-23T13:02:14Z|q-fin
http://arxiv.org/abs/1410.5466v4|Conditional Preference Orders and their Numerical Representations|Samuel Drapeau;Asgar Jamneshan|q-fin.EC|We provide an axiomatic system modeling conditional preference orders which is based on conditional set theory. Conditional numerical representations are introduced, and a conditional version of the theorems of Debreu on the existence of numerical representations is proved. The conditionally continuous representations follow from a conditional version of Debreu's Gap Lemma the proof of which relies on a conditional version of the axiom of choice, free of any measurable selection argument. We give a conditional version of the von Neumann and Morgenstern representation as well as automatic conditional continuity results, and illustrate them by examples.|2014-10-20T21:09:01Z|2016-01-18T17:35:54Z|q-fin
http://arxiv.org/abs/1104.5131v2|American Options Based on Malliavin Calculus and Nonparametric Variance   Reduction Methods|Lokman Abbas-Turki;Bernard Lapeyre|q-fin.PR|This paper is devoted to pricing American options using Monte Carlo and the Malliavin calculus. Unlike the majority of articles related to this topic, in this work we will not use localization fonctions to reduce the variance. Our method is based on expressing the conditional expectation E[f(St)/Ss] using the Malliavin calculus without localization. Then the variance of the estimator of E[f(St)/Ss] is reduced using closed formulas, techniques based on a conditioning and a judicious choice of the number of simulated paths. Finally, we perform the stopping times version of the dynamic programming algorithm to decrease the bias. On the one hand, we will develop the Malliavin calculus tools for exponential multi-dimensional diffusions that have deterministic and no constant coefficients. On the other hand, we will detail various nonparametric technics to reduce the variance. Moreover, we will test the numerical efficiency of our method on a heterogeneous CPU/GPU multi-core machine.|2011-04-27T13:03:19Z|2011-04-28T06:22:46Z|q-fin
http://arxiv.org/abs/1901.03874v2|A Risk-Sharing Framework of Bilateral Contracts|Junbeom Lee;Stephan Sturm;Chao Zhou|q-fin.MF|We introduce a two-agent problem which is inspired by price asymmetry arising from funding difference. When two parties have different funding rates, the two parties deduce different fair prices for derivative contracts even under the same pricing methodology and parameters. Thus, the two parties should enter the derivative contracts with a negotiated price, and we call the negotiation a risk-sharing problem. This framework defines the negotiation as a problem that maximizes the sum of utilities of the two parties. By the derived optimal price, we provide a theoretical analysis on how the price is determined between the two parties. As well as the price, the risk-sharing framework produces an optimal amount of collateral. The derived optimal collateral can be used for contracts between financial firms and non-financial firms. However, inter-dealers markets are governed by regulations. As recommended in Basel III, it is a convention in inter-dealer contracts to pledge the full amount of a close-out price as collateral. In this case, using the optimal collateral, we interpret conditions for the full margin requirement to be indeed optimal.|2019-01-12T15:45:41Z|2019-12-29T04:43:42Z|q-fin
http://arxiv.org/abs/physics/0408100v1|More on the early interpretation of the Schwarzschild solution|Andre Gsponer|physics.hist-ph|Lemaitre was apparently the first to make an explicit coordinate transformation resulting in the removal of the singularity at r = a = 2m in the Schwarzschild metric, while Lanczos was the first to express doubts on the physical reality of that singularity since it could be introduced or removed by a transformation of coordinates.|2004-08-23T16:14:59Z|2004-08-23T16:14:59Z|physics
http://arxiv.org/abs/physics/0607164v1|Dynamics and (de)localization in a one-dimensional tight-binding chain|Antonio Siber|physics.ed-ph|A simple tight-binding model is used to illustrate how the time dependence of a state vector can be obtained from all the eigenvalues and eigenvectors of the Hamiltonian. The behavior of the eigenvalues and eigenvectors is studied for various parameters and allows us to study scattering-like events, impurity states, and localization in disordered systems.|2006-07-18T11:20:42Z|2006-07-18T11:20:42Z|physics
http://arxiv.org/abs/1701.02265v1|On Reject and Refine Options in Multicategory Classification|Chong Zhang;Wenbo Wang;Xingye Qiao|stat.ML|In many real applications of statistical learning, a decision made from misclassification can be too costly to afford; in this case, a reject option, which defers the decision until further investigation is conducted, is often preferred. In recent years, there has been much development for binary classification with a reject option. Yet, little progress has been made for the multicategory case. In this article, we propose margin-based multicategory classification methods with a reject option. In addition, and more importantly, we introduce a new and unique refine option for the multicategory problem, where the class of an observation is predicted to be from a set of class labels, whose cardinality is not necessarily one. The main advantage of both options lies in their capacity of identifying error-prone observations. Moreover, the refine option can provide more constructive information for classification by effectively ruling out implausible classes. Efficient implementations have been developed for the proposed methods. On the theoretical side, we offer a novel statistical learning theory and show a fast convergence rate of the excess $\ell$-risk of our methods with emphasis on diverging dimensionality and number of classes. The results can be further improved under a low noise assumption. A set of comprehensive simulation and real data studies has shown the usefulness of the new learning tools compared to regular multicategory classifiers. Detailed proofs of theorems and extended numerical results are included in the supplemental materials available online.|2017-01-09T17:19:45Z|2017-01-09T17:19:45Z|stat
http://arxiv.org/abs/1608.02738v1|Reliability Considerations for the Operation of Large Accelerator User   Facilities|F. J. Willeke|physics.acc-ph|The lecture provides an overview of considerations relevant for achieving highly reliable operation of accelerator based user facilities. The article starts with an overview of statistical reliability formalism which is followed by high reliability design considerations with examples. The article closes with operational aspects of high reliability such as preventive maintenance and spares inventory.|2016-08-09T09:30:12Z|2016-08-09T09:30:12Z|physics
http://arxiv.org/abs/1406.0818v1|Two planets around Kapteyn's star : a cold and a temperate super-Earth   orbiting the nearest halo red-dwarf|Guillem Anglada-Escudé;Pamela Arriagada;Mikko Tuomi;Mathias Zechmeister;James S. Jenkins;Aviv Ofir;Stefan Dreizler;Enrico Gerlach;Chris J. Marvin;Ansgar Reiners;Sandra V. Jeffers;R. Paul Butler;Steven S. Vogt;Pedro J. Amado;Cristina Rodríguez-López;Zaira M. Berdiñas;Julian Morin;Jeff D. Crane;Stephen A. Shectman;Ian B. Thompson;Matías Díaz;Eugenio Rivera;Luis F. Sarmiento;Hugh R. A. Jones|astro-ph.EP|Exoplanets of a few Earth masses can be now detected around nearby low-mass stars using Doppler spectroscopy. In this paper, we investigate the radial velocity variations of Kapteyn's star, which is both a sub-dwarf M-star and the nearest halo object to the Sun. The observations comprise archival and new HARPS, HIRES and PFS Doppler measurements. Two Doppler signals are detected at periods of 48 and 120 days using likelihood periodograms and a Bayesian analysis of the data. Using the same techniques, the activity indicies and archival ASAS-3 photometry show evidence for low-level activity periodicities of the order of several hundred days. However, there are no significant correlations with the radial velocity variations on the same time-scales. The inclusion of planetary Keplerian signals in the model results in levels of correlated and excess white noise that are remarkably low compared to younger G, K and M dwarfs. We conclude that Kapteyn's star is most probably orbited by two super-Earth mass planets, one of which is orbiting in its circumstellar habitable zone, becoming the oldest potentially habitable planet known to date. The presence and long-term survival of a planetary system seems a remarkable feat given the peculiar origin and kinematic history of Kapteyn's star. The detection of super-Earth mass planets around halo stars provides important insights into planet-formation processes in the early days of the Milky Way.|2014-06-03T19:18:40Z|2014-06-03T19:18:40Z|astro-ph
http://arxiv.org/abs/1406.5312v1|Asymptotic Exponential Arbitrage and Utility-based Asymptotic Arbitrage   in Markovian Models of Financial Markets|Martin Le Doux Mbele Bidima;Miklós Rásonyi|math.OC|Consider a discrete-time infinite horizon financial market model in which the logarithm of the stock price is a time discretization of a stochastic differential equation. Under conditions different from those given in a previous paper of ours, we prove the existence of investment opportunities producing an exponentially growing profit with probability tending to $1$ geometrically fast. This is achieved using ergodic results on Markov chains and tools of large deviations theory.   Furthermore, we discuss asymptotic arbitrage in the expected utility sense and its relationship to the first part of the paper.|2014-06-20T08:36:04Z|2014-06-20T08:36:04Z|math
http://arxiv.org/abs/1101.0943v1|Leo and me|Jacob Feldman|stat.AP|"I arrived in Berkeley in 1957, at which time Leo was an Acting Assistant Professor of Mathematics here. He had recently proven the ""individual ergodic theorem of information theory""---a triumph---and since this was becoming central to my own interests, it would have been natural for us to work together. However, Leo's interests shifted to more applied work, specifically statistics, and he soon moved to UCLA. So we never became collaborators, but we did became good friends, especially after 1980 when he returned to Berkeley as a Professor of Statistics."|2011-01-05T11:59:01Z|2011-01-05T11:59:01Z|stat
http://arxiv.org/abs/1605.05685v1|First direct evidence of two stages in free recall and three   corresponding estimates of working memory capacity|Eugen Tarnow|q-bio.OT|I find that exactly two stages can be seen directly in sequential free recall distributions. These distributions show that the first three recalls come from the emptying of working memory, recalls 6 and above come from a second stage and the 4th and 5th recalls are mixtures of the two. A discontinuity, a rounded step function, is shown to exist in the fitted linear slope of the recall distributions as the recall shifts from the emptying of working memory (positive slope) to the second stage (negative slope). The discontinuity leads to a first estimate of the capacity of working memory at 4-4.5 items. Working memory accounts for the recency effect. The primacy effect comes from the second stage with a contribution also from working memory for short lists (the first item). The different slopes of the working memory and secondary stages, and that the two have different functional forms, accounts for the u-shaped serial position curve. The total recall is shown to be a linear combination of the content of working memory and items recalled in the second stage with 3.0-3.9 items coming from working memory, a second estimate of the capacity of working memory. A third, separate upper limit on the capacity of working memory is found (3.06 items), corresponding to the requirement that the content of working memory cannot exceed the total recall, item by item. This third limit presumably corresponds to the least chunked item. This is the best limit on the capacity of unchunked working memory.|2016-04-06T11:17:03Z|2016-04-06T11:17:03Z|q-bio
http://arxiv.org/abs/1901.04500v1|On the Prevalence of Super-Massive Black Holes over Cosmic Time|Johannes Buchner;Ezequiel Treister;Franz E. Bauer;Lia F. Sartori;Kevin Schawinski|astro-ph.GA|We investigate the abundance of Super-Massive Black Hole (SMBH) seeds in primordial galaxy halos. We explore the assumption that dark matter halos outgrowing a critical halo mass M_c have some probability p of having spawned a SMBH seed. Current observations of local, intermediate-mass galaxies constrain these parameters: For $M_c=10^{11}M_\odot$, all halos must be seeded, but when adopting smaller M_c masses the seeding can be much less efficient. The constraints also put lower limits on the number density of black holes in the local and high-redshift Universe. Reproducing z~6 quasar space densities depends on their typical halo mass, which can be constrained by counting nearby Lyman Break Galaxies and Lyman Alpha Emitters. For both observables, our simulations demonstrate that single-field predictions are too diverse to make definitive statements, in agreement with mixed claims in the literature. If quasars are not limited to the most massive host halos, they may represent a tiny fraction (~10^-5) of the SMBH population. Finally, we produce a wide range of predictions for gravitational events from SMBH mergers. We define a new diagnostic diagram for LISA to measure both SMBH space density and the typical delay between halo merger and black hole merger. While previous works have explored specific scenarios, our results hold independent of the seed mechanism, seed mass, obscuration, fueling methods and duty cycle.|2019-01-14T19:00:02Z|2019-01-14T19:00:02Z|astro-ph
http://arxiv.org/abs/1003.2329v1|Why is life so exact?|Denis A. Semenov|physics.bio-ph|"This is my attempt to answer Schr\""odinger's question: ""What is Life?"". In living cell the local combination of atoms is reproduced with incredible accuracy. In the case of protein biosynthesis the notion of the physical aspect of the process can be qualitatively enriched by taking into account the symmetry of conformational oscillations. This is the key to understand the selectivity in biochemistry."|2010-03-11T13:22:40Z|2010-03-11T13:22:40Z|physics
http://arxiv.org/abs/0711.0409v1|Dynamic Moment Analysis of the Extracellular Electric Field of a   Biologically Realistic Spiking Neuron|J. N. Milstein;Christof Koch|q-bio.NC|Based upon the membrane currents generated by an action potential in a biologically realistic model of a pyramidal, hippocampal cell within rat CA1, we perform a moment expansion of the extracellular field potential. We decompose the potential into both inverse and classical moments and show that this method is a rapid and efficient way to calculate the extracellular field both near and far from the cell body. The action potential gives rise to a large quadrupole moment that contributes to the extracellular field up to distances of almost 1 cm. This method will serve as a starting point in connecting the microscopic generation of electric fields at the level of neurons to macroscopic observables such as the local field potential.|2007-11-02T22:57:06Z|2007-11-02T22:57:06Z|q-bio
http://arxiv.org/abs/1503.01842v1|CEoptim: Cross-Entropy R Package for Optimization|Tim Benham;Qibin Duan;Dirk P. Kroese;Benoit Liquet|stat.CO|The cross-entropy (CE) method is simple and versatile technique for optimization, based on Kullback-Leibler (or cross-entropy) minimization. The method can be applied to a wide range of optimization tasks, including continuous, discrete, mixed and constrained optimization problems. The new package CEoptim provides the R implementation of the CE method for optimization. We describe the general CE methodology for optimization and well as some useful modifications. The usage and efficacy of CEoptim is demonstrated through a variety of optimization examples, including model fitting, combinatorial optimization, and maximum likelihood estimation.|2015-03-06T03:43:34Z|2015-03-06T03:43:34Z|stat
http://arxiv.org/abs/1506.06669v3|Understanding the Impact of Microcredit Expansions: A Bayesian   Hierarchical Analysis of 7 Randomised Experiments|Rachael Meager|q-fin.EC|"Bayesian hierarchical models are a methodology for aggregation and synthesis of data from heterogeneous settings, used widely in statistics and other disciplines. I apply this framework to the evidence from 7 randomized experiments of expanding access to microcredit to assess the general impact of the intervention on household outcomes and the heterogeneity in this impact across sites. The results suggest that the effect of microcredit is likely to be positive but small relative to control group average levels, and the possibility of a negative impact cannot be ruled out. By contrast, common meta-analytic methods that pool all the data without assessing the heterogeneity misleadingly produce ""statistically significant"" results in 2 of the 6 household outcomes. Standard pooling metrics for the studies indicate on average 60% pooling on the treatment effects, suggesting that the site-specific effects are reasonably externally valid, and thus informative for each other and for the general case. The cross-study heterogeneity is almost entirely generated by heterogeneous effects for the 27% households who previously operated businesses before microcredit expansion, although this group is likely to see much larger impacts overall. A Ridge regression procedure to assess the correlations between site-specific covariates and treatment effects indicates that the remaining heterogeneity is strongly correlated with differences in economic variables, but not with differences in study design protocols. The average interest rate and the average loan size have the strongest correlation with the treatment effects, and both are negative."|2015-06-22T16:23:51Z|2016-07-12T20:28:38Z|q-fin
http://arxiv.org/abs/1002.3820v3|Conservative Constraints on Dark Matter from the Fermi-LAT Isotropic   Diffuse Gamma-Ray Background Spectrum|Kevork N. Abazajian;Prateek Agrawal;Zackaria Chacko;Can Kilic|astro-ph.HE|We examine the constraints on final state radiation from Weakly Interacting Massive Particle (WIMP) dark matter candidates annihilating into various standard model final states, as imposed by the measurement of the isotropic diffuse gamma-ray background by the Large Area Telescope aboard the Fermi Gamma-Ray Space Telescope. The expected isotropic diffuse signal from dark matter annihilation has contributions from the local Milky Way (MW) as well as from extragalactic dark matter. The signal from the MW is very insensitive to the adopted dark matter profile of the halos, and dominates the signal from extragalactic halos, which is sensitive to the low mass cut-off of the halo mass function. We adopt a conservative model for both the low halo mass survival cut-off and the substructure boost factor of the Galactic and extragalactic components, and only consider the primary final state radiation. This provides robust constraints which reach the thermal production cross-section for low mass WIMPs annihilating into hadronic modes. We also reanalyze limits from HESS observations of the Galactic Ridge region using a conservative model for the dark matter halo profile. When combined with the HESS constraint, the isotropic diffuse spectrum rules out all interpretations of the PAMELA positron excess based on dark matter annihilation into two lepton final states. Annihilation into four leptons through new intermediate states, although constrained by the data, is not excluded.|2010-02-19T20:59:25Z|2010-10-28T15:37:42Z|astro-ph
http://arxiv.org/abs/0906.5581v2|Strong Taylor approximation of stochastic differential equations and   application to the Lévy LIBOR model|Antonis Papapantoleon;Maria Siopacha|math.PR|In this article we develop a method for the strong approximation of stochastic differential equations (SDEs) driven by L\'evy processes or general semimartingales. The main ingredients of our method is the perturbation of the SDE and the Taylor expansion of the resulting parameterized curve. We apply this method to develop strong approximation schemes for LIBOR market models. In particular, we derive fast and precise algorithms for the valuation of derivatives in LIBOR models which are more tractable than the simulation of the full SDE. A numerical example for the L\'evy LIBOR model illustrates our method.|2009-06-30T16:40:48Z|2010-10-04T12:37:39Z|math
http://arxiv.org/abs/1412.1183v2|Regulatory Capital Modelling for Credit Risk|Marek Rutkowski;Silvio Tarca|q-fin.RM|The Basel II internal ratings-based (IRB) approach to capital adequacy for credit risk plays an important role in protecting the Australian banking sector against insolvency. We outline the mathematical foundations of regulatory capital for credit risk, and extend the model specification of the IRB approach to a more general setting than the usual Gaussian case. It rests on the proposition that quantiles of the distribution of conditional expectation of portfolio percentage loss may be substituted for quantiles of the portfolio loss distribution. We present a more economical proof of this proposition under weaker assumptions. Then, constructing a portfolio that is representative of credit exposures of the Australian banking sector, we measure the rate of convergence, in terms of number of obligors, of empirical loss distributions to the asymptotic (infinitely fine-grained) portfolio loss distribution. Moreover, we evaluate the sensitivity of credit risk capital to dependence structure as modelled by asset correlations and elliptical copulas. Access to internal bank data collected by the prudential regulator distinguishes our research from other empirical studies on the IRB approach.|2014-12-03T04:25:48Z|2016-07-07T09:34:39Z|q-fin
http://arxiv.org/abs/1310.5791v3|ROP: Matrix recovery via rank-one projections|T. Tony Cai;Anru Zhang|math.ST|Estimation of low-rank matrices is of significant interest in a range of contemporary applications. In this paper, we introduce a rank-one projection model for low-rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low-rank matrices in the noisy case. The procedure is adaptive to the rank and robust against small perturbations. Both upper and lower bounds for the estimation accuracy under the Frobenius norm loss are obtained. The proposed estimator is shown to be rate-optimal under certain conditions. The estimator is easy to implement via convex programming and performs well numerically. The techniques and main results developed in the paper also have implications to other related statistical problems. An application to estimation of spiked covariance matrices from one-dimensional random projections is considered. The results demonstrate that it is still possible to accurately estimate the covariance matrix of a high-dimensional distribution based only on one-dimensional projections.|2013-10-22T03:30:29Z|2014-12-09T10:24:46Z|math
http://arxiv.org/abs/1510.01305v1|Astroparticle Physics at Eastern Colombia|Hernan Asorey;Luis A. Nunez|astro-ph.IM|We present the emerging panorama of Astroparticle Physics at Eastern Colombia, and describe several ongoing projects, most of them related to the Latin American Giant Observatory (LAGO) Project. This research work is carried out at the Grupo de Investigaciones en Relatividad y Gravitaci\'on of Universidad Industrial de Santander.|2015-10-05T19:49:23Z|2015-10-05T19:49:23Z|astro-ph
http://arxiv.org/abs/1604.08921v2|An ecological resilience perspective on cancer: insights from a toy   model|Artur C. Fassoni;Hyun M. Yang|q-bio.PE|In this paper we propose an ecological resilience point of view on cancer. This view is based on the analysis of a simple ODE model for the interactions between cancer and normal cells. The model presents two regimes for tumor growth. In the first, cancer arises due to three reasons: a partial corruption of the functions that avoid the growth of mutated cells, an aggressive phenotype of tumor cells and exposure to external carcinogenic factors. In this case, treatments may be effective if they drive the system to the basin of attraction of the cancer cure state. In the second regime, cancer arises because the repair system is intrinsically corrupted. In this case, the complete cure is not possible since the cancer cure state is no more stable, but tumor recurrence may be delayed if treatment is prolongued. We review three indicators of the resilience of a stable equilibrium, related with size and shape of its basin of attraction: latitude, precariousness and resistance. A novel method to calculate these indicators is proposed. This method is simpler and more efficient than those currently used, and may be easily applied to other population dynamics models. We apply this method to the model and investigate how these indicators behave with parameters changes. Finally, we present some simulations to illustrate how the resilience analysis can be applied to validated models in order to obtain indicators for personalized cancer treatments.   Keywords: Tumor growth; Chemotherapy; Basins of Attraction; Regime shifts; Critical transitions|2016-04-29T17:43:41Z|2016-08-31T19:37:47Z|q-bio
http://arxiv.org/abs/1803.04353v3|Partial Identifiability of Restricted Latent Class Models|Yuqi Gu;Gongjun Xu|math.ST|Latent class models have wide applications in social and biological sciences. In many applications, pre-specified restrictions are imposed on the parameter space of latent class models, through a design matrix, to reflect practitioners' assumptions about how the observed responses depend on subjects' latent traits. Though widely used in various fields, such restricted latent class models suffer from non-identifiability due to their discreteness nature and complex structure of restrictions. This work addresses the fundamental identifiability issue of restricted latent class models by developing a general framework for strict and partial identifiability of the model parameters. Under correct model specification, the developed identifiability conditions only depend on the design matrix and are easily checkable, which provide useful practical guidelines for designing statistically valid diagnostic tests. Furthermore, the new theoretical framework is applied to establish, for the first time, identifiability of several designs from cognitive diagnosis applications.|2018-03-12T16:23:08Z|2019-05-31T01:20:42Z|math
http://arxiv.org/abs/math/0607466v1|Global Stabilization of a Class of Partially Known Positive Systems|Jean-Luc Gouzé;Olivier Bernard;Ludovic Mailleret|math.OC|In this report we deal with the problem of global output feedback stabilization of a class of $n$-dimensional nonlinear positive systems possessing a one-dimensional unknown, though measured, part. We first propose our main result, an output feedback control procedure, taking advantage of measurements of the uncertain part, able to globally stabilize the system towards an adjustable equilibrium point in the interior of the positive orthant. Though quite general, this result is based on hypotheses that might be difficult to check in practice. Then in a second step, through a Theorem on a class of positive systems linking the existence of a strongly positive equillibrium to its global asymptotic stability, we propose other hypotheses for our main result to hold. These new hypotheses are more restrictive but much simpler to check. Some illustrative examples, highlighting both the potential complex open loop dynamics (multi-stability, limit cycle, chaos) of the considered systems and the interest of the control procedure, conclude this report.|2006-07-19T15:02:26Z|2006-07-19T15:02:26Z|math
http://arxiv.org/abs/1205.6473v2|The Stellar Initial Mass Function in Early-Type Galaxies From Absorption   Line Spectroscopy. II. Results|Charlie Conroy;Pieter van Dokkum|astro-ph.CO|The spectral absorption lines in early-type galaxies contain a wealth of information regarding the detailed abundance pattern, star formation history, and stellar initial mass function (IMF) of the underlying stellar population. Using our new population synthesis model that accounts for the effect of variable abundance ratios of 11 elements, we analyze very high quality absorption line spectra of 38 early-type galaxies and the nuclear bulge of M31. These data extend to 1um and they therefore include the IMF-sensitive spectral features NaI, CaII, and FeH at 0.82um, 0.86um and 0.99um, respectively. The models fit the data well, with typical rms residuals ~1%. Strong constraints on the IMF and therefore the stellar mass-to-light ratio, (M/L)_stars, are derived for individual galaxies. We find that the IMF becomes increasingly bottom-heavy with increasing velocity dispersion and [Mg/Fe]. At the lowest dispersions and [Mg/Fe] values the derived IMF is consistent with the Milky Way IMF, while at the highest dispersions and [Mg/Fe] values the derived IMF contains more low-mass stars (is more bottom-heavy) than even a Salpeter IMF. Our best-fit (M/L)_stars values do not exceed dynamically-based M/L values. We also apply our models to stacked spectra of four metal-rich globular clusters in M31 and find an (M/L)_stars that implies fewer low-mass stars than a Milky Way IMF, again agreeing with dynamical constraints. We discuss other possible explanations for the observed trends and conclude that variation in the IMF is the simplest and most plausible.|2012-05-29T20:00:02Z|2012-10-12T03:34:25Z|astro-ph
http://arxiv.org/abs/1812.06533v2|What Is the Value Added by Using Causal Machine Learning Methods in a   Welfare Experiment Evaluation?|Anthony Strittmatter|econ.EM|Recent studies have proposed causal machine learning (CML) methods to estimate conditional average treatment effects (CATEs). In this study, I investigate whether CML methods add value compared to conventional CATE estimators by re-evaluating Connecticut's Jobs First welfare experiment. This experiment entails a mix of positive and negative work incentives. Previous studies show that it is hard to tackle the effect heterogeneity of Jobs First by means of CATEs. I report evidence that CML methods can provide support for the theoretical labor supply predictions. Furthermore, I document reasons why some conventional CATE estimators fail and discuss the limitations of CML methods.|2018-12-16T20:24:02Z|2019-03-16T21:13:51Z|econ
http://arxiv.org/abs/1512.03618v1|Macroeconomic Dynamics of Assets, Leverage and Trust|Jeroen Rozendaal;Yannick Malevergne;Didier Sornette|q-fin.EC|A macroeconomic model based on the economic variables (i) assets, (ii) leverage (defined as debt over asset) and (iii) trust (defined as the maximum sustainable leverage) is proposed to investigate the role of credit in the dynamics of economic growth, and how credit may be associated with both economic performance and confidence. Our first notable finding is the mechanism of reward/penalty associated with patience, as quantified by the return on assets. In regular economies where the EBITA/Assets ratio is larger than the cost of debt, starting with a trust higher than leverage results in the highest long-term return on assets (which can be seen as a proxy for economic growth). Our second main finding concerns a recommendation for the reaction of a central bank to an external shock that affects negatively the economic growth. We find that late policy intervention in the model economy results in the highest long-term return on assets and largest asset value. But this comes at the cost of suffering longer from the crisis until the intervention occurs. The phenomenon can be ascribed to the fact that postponing intervention allows trust to increase first, and it is most effective to intervene when trust is high. These results derive from two fundamental assumptions underlying our model: (a) trust tends to increase when it is above leverage; (b) economic agents learn optimally to adjust debt for a given level of trust and amount of assets. Using a Markov Switching Model for the EBITA/Assets ratio, we have successfully calibrated our model to the empirical data of the return on equity of the EURO STOXX 50 for the time period 2000-2013. We find that dynamics of leverage and trust can be highly non-monotonous with curved trajectories, as a result of the nonlinear coupling between the variables.|2015-12-11T12:19:50Z|2015-12-11T12:19:50Z|q-fin
http://arxiv.org/abs/1208.3553v1|The Dependence of Routine Bayesian Model Selection Methods on Irrelevant   Alternatives|Piotr Zwiernik;Jim Q. Smith|stat.ME|"Bayesian methods - either based on Bayes Factors or BIC - are now widely used for model selection. One property that might reasonably be demanded of any model selection method is that if a model ${M}_{1}$ is preferred to a model ${M}_{0}$, when these two models are expressed as members of one model class $\mathbb{M}$, this preference is preserved when they are embedded in a different class $\mathbb{M}'$. However, we illustrate in this paper that with the usual implementation of these common Bayesian procedures this property does not hold true even approximately. We therefore contend that to use these methods it is first necessary for there to exist a ""natural"" embedding class. We argue that in any context like the one illustrated in our running example of Bayesian model selection of binary phylogenetic trees there is no such embedding."|2012-08-17T08:20:31Z|2012-08-17T08:20:31Z|stat
http://arxiv.org/abs/1501.01898v1|Fast Estimation of Diffusion Tensors under Rician noise by the EM   algorithm|Jia Liu;Dario Gasbarra;Juha Railavo|stat.CO|This paper presents a fast computational method, the Expectation Maximization algorithm, for Maximum Likelihood (ML) estimation in diffusion tensor imaging under the Rice noise model. We further extend the ML framework to the maximum a posterior (MAP) estimation and describe the numerical similarities of both ML and MAP estimators. This novel method is implemented and applied using both synthetic and real data in a wide range of b amplitudes. The comparison with other popular methods are made in accuracy, methodology and computation.|2015-01-08T16:29:16Z|2015-01-08T16:29:16Z|stat
http://arxiv.org/abs/1401.8115v1|Inhomogeneous K-function for germ-grain models|M. Ángeles Gallego;M. Victoria Ibáñez;Amelia Simó|stat.OT|In this paper, we propose a generalization to germ-grain models of the inhomogeneous K-function of Point Processes. We apply them to a sample of images of peripheral blood smears obtained from patients with Sickle Cell Disease, in order to decide whether the sample belongs to the thin, thick or morphological region.|2014-01-31T10:25:44Z|2014-01-31T10:25:44Z|stat
http://arxiv.org/abs/1507.05990v2|Estimation and uncertainty of reversible Markov models|Benjamin Trendelkamp-Schroer;Hao Wu;Fabian Paul;Frank Noé|physics.chem-ph|Reversibility is a key concept in Markov models and Master-equation models of molecular kinetics. The analysis and interpretation of the transition matrix encoding the kinetic properties of the model relies heavily on the reversibility property. The estimation of a reversible transition matrix from simulation data is therefore crucial to the successful application of the previously developed theory. In this work we discuss methods for the maximum likelihood estimation of transition matrices from finite simulation data and present a new algorithm for the estimation if reversibility with respect to a given stationary vector is desired. We also develop new methods for the Bayesian posterior inference of reversible transition matrices with and without given stationary vector taking into account the need for a suitable prior distribution preserving the meta- stable features of the observed process during posterior inference. All algorithms here are implemented in the PyEMMA software - http://pyemma.org - as of version 2.0.|2015-07-19T21:38:45Z|2015-09-22T03:13:42Z|physics
http://arxiv.org/abs/1309.6551v2|Neutral genomic regions refine models of recent rapid human population   growth|Elodie Gazave;Li Ma;Diana Chang;Alex Coventry;Feng Gao;Donna Muzny;Eric Boerwinkle;Richard Gibbs;Charles F. Sing;Andrew G. Clark;Alon Keinan|q-bio.PE|Human populations have experienced dramatic growth since the Neolithic revolution. Recent studies that sequenced a very large number of individuals observed an extreme excess of rare variants, and provided clear evidence of recent rapid growth in effective population size, though estimates have varied greatly among studies. All these studies were based on protein-coding genes, in which variants are also impacted by natural selection. In this study, we introduce targeted sequencing data for studying recent human history with minimal confounding by natural selection. We sequenced loci very far from genes that meet a wide array of additional criteria such that mutations in these loci are putatively neutral. As population structure also skews allele frequencies, we sequenced a sample of relatively homogeneous ancestry by first analyzing the population structure of 9,716 European Americans. We employed very high coverage sequencing to reliably call rare variants, and fit an extensive array of models of recent European demographic history to the site frequency spectrum. The best-fit model estimates ~3.4% growth per generation during the last ~140 generations, resulting in a population size increase of two orders of magnitude. This model fits the data very well, largely due to our observation that assumptions of more ancient demography can impact estimates of recent growth. This observation and results also shed light on the discrepancy in demographic estimates among recent studies.|2013-09-25T16:01:10Z|2013-11-15T22:36:23Z|q-bio
http://arxiv.org/abs/1905.02543v1|Chromatic Aberration in Metalenses|Cristian E. Gutiérrez;Ahmad Sabra|physics.class-ph|This paper provides a mathematical approach to study chromatic aberration in metalenses. It is shown that radiation of a given wavelength is refracted according to a generalized Snell's law which together with the notion of envelope yields the existence of phase discontinuities. This is then used to establish a quantitative measure of dispersion in metalenses concluding that in the visible spectrum it has the same order of magnitude as for standard lenses.|2019-05-03T15:42:24Z|2019-05-03T15:42:24Z|physics
http://arxiv.org/abs/1604.01329v6|DNA to DNA transcription might exist in eukaryotic cells|Gao-De Li|q-bio.SC|Till now, in biological sciences, the term, transcription, mainly refers to DNA to RNA transcription. But our recently published experimental findings obtained from Plasmodium falciparum strongly suggest the existence of DNA to DNA transcription in the genome of eukaryotic cells, which could shed some light on the functions of certain noncoding DNA in the human and other eukaryotic genomes.|2016-04-05T16:56:51Z|2018-01-29T09:01:57Z|q-bio
http://arxiv.org/abs/0911.0596v1|Ab initio computational modeling of tumor spheroids|Roberto Chignola;Alessio Del Fabbro;Marcello Farina;Edoardo Milotti|q-bio.CB|This paper is a review of the VBL project, where we develop and test a numerical simulator of tumor spheroids.|2009-11-03T15:41:49Z|2009-11-03T15:41:49Z|q-bio
http://arxiv.org/abs/physics/9909009v1|Fundamental Physical Constants and the Principle of Parametric   Incompleteness|S. S. Stepanov|physics.gen-ph|The principle which allows to construct new physical theories on the basis of classical mechanics by reduction of the number of its axiom without engaging new postulates is formulated. The arising incompleteness of theory manifests itself in terms of theoretically undefinable fundamental physical constants $\hbar$ and c. As an example we built up a parametric generalization of relativistic theory, where the Hubble Law and the dependence of light velocity on time are obtained.|1999-09-07T07:56:24Z|1999-09-07T07:56:24Z|physics
http://arxiv.org/abs/1904.11583v2|Time-dependent product-form Poisson distributions for reaction networks   with higher order complexes|David F. Anderson;David Schnoerr;Chaojie Yuan|math.PR|"It is well known that stochastically modeled reaction networks that are complex balanced admit a stationary distribution that is a product of Poisson distributions. In this paper, we consider the following related question: supposing that the initial distribution of a stochastically modeled reaction network is a product of Poissons, under what conditions will the distribution remain a product of Poissons for all time? By drawing inspiration from Crispin Gardiner's ""Poisson representation"" for the solution to the chemical master equation, we provide a necessary and sufficient condition for such a product-form distribution to hold for all time. Interestingly, the condition is a dynamical ""complex-balancing"" for only those complexes that have multiplicity greater than or equal to two (i.e. the higher order complexes that yield non-linear terms to the dynamics). We term this new condition the ""dynamical and restricted complex balance"" condition (DR for short)."|2019-04-25T20:58:10Z|2019-11-18T17:11:32Z|math
http://arxiv.org/abs/1407.1774v1|gamboostLSS: An R Package for Model Building and Variable Selection in   the GAMLSS Framework|Benjamin Hofner;Andreas Mayr;Matthias Schmid|stat.CO|Generalized additive models for location, scale and shape (GAMLSS) are a flexible class of regression models that allow to model multiple parameters of a distribution function, such as the mean and the standard deviation, simultaneously. With the R package gamboostLSS, we provide a boosting method to fit these models. Variable selection and model choice are naturally available within this regularized regression framework. To introduce and illustrate the R package gamboostLSS and its infrastructure, we use a data set on stunted growth in India. In addition to the specification and application of the model itself, we present a variety of convenience functions, including methods for tuning parameter selection, prediction and visualization of results. The package gamboostLSS is available from CRAN (http://cran.r-project.org/package=gamboostLSS).|2014-07-07T17:11:09Z|2014-07-07T17:11:09Z|stat
